kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: kube-system
  labels:
    app: filebeat
apiVersion: v1
data:
  filebeat.yml: |-
    filebeat.inputs:

    # Each - is an input. Most options can be set at the input level, so
    # you can use different inputs for various configurations.
    # Below are the input specific configurations.
    # https://www.elastic.co/blog/enrich-docker-logs-with-filebeat
    - type: docker
      containers.ids:
      - "*"
      tags: ["k8s"]
      fields:
        log_topic: k8s

      processors:
        - add_kubernetes_metadata:
            in_cluster: true

    #============================= Filebeat modules ===============================

    filebeat.config.modules:
      # Glob pattern for configuration loading
      path: ${path.config}/modules.d/*.yml

      # Set to true to enable config reloading
      reload.enabled: false

      # Period on which files under path should be checked for changes
      #reload.period: 10s

    #==================== Elasticsearch template setting ==========================

    setup.template.settings:
      index.number_of_shards: 3
      #index.codec: best_compression
      #_source.enabled: false

    #----------------------------- Logstash output --------------------------------
    #output.logstash:
      # The Logstash hosts
      #hosts: ["logstash:5044"]

      # Optional SSL. By default is off.
      # List of root certificates for HTTPS server verifications
      # ssl.certificate_authorities: ["/etc/pki/tls/certs/logstash-beats.crt"]

      # Certificate for SSL client authentication
      #ssl.certificate: "/etc/pki/client/cert.pem"

      # Client Certificate Key
      #ssl.key: "/etc/pki/client/cert.key"

    #-------------------------- Elasticsearch output ------------------------------
    # output.elasticsearch:
      # Array of hosts to connect to.
      # hosts: ["elasticsearch:9200"]

    #----------------------------- Kafka output --------------------------------
    output.kafka:
      #initial brokers for reading cluster metadata
      #enable: false
      hosts: ["bootstrap.kafka.svc.cluster.local:9092"]

      #message topic selection + partitioning
      topic: '%{[fields.log_topic]}'
      partition.round_robin:
        reachable_only: false

      required_acks: 1
      compression: gzip
      max_message_bytes: 1000000
